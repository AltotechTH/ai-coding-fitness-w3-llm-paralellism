{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Demo: How Do You Run a Model That Doesn't Fit on a GPU?\n",
    "\n",
    "**Model**: Qwen2.5 72B Instruct (72 billion parameters)  \n",
    "**Hardware**: 2x NVIDIA A100 80GB (SXM)  \n",
    "\n",
    "We have a problem: this model is 144GB, but each GPU only has 80GB of memory.  \n",
    "Let's figure out how to make it work -- and compare different approaches.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup\n",
    "\n",
    "Install libraries, log in to HuggingFace, and download the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0 Clean GPU slate\n",
    "\n",
    "Run this FIRST every time you start (or restart) the notebook.  \n",
    "Kills any leftover processes from previous runs that might be hogging GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No leftover GPU processes found.\n",
      "index, memory.used [MiB], memory.total [MiB]\n",
      "0, 1 MiB, 81920 MiB\n",
      "1, 1 MiB, 81920 MiB\n"
     ]
    }
   ],
   "source": [
    "import subprocess, os, signal\n",
    "\n",
    "def nuke_gpu_processes():\n",
    "    \"\"\"Kill ALL processes using GPUs (except this Jupyter kernel).\"\"\"\n",
    "    my_pid = os.getpid()\n",
    "    killed = []\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"nvidia-smi\", \"--query-compute-apps=pid\", \"--format=csv,noheader\"],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        for line in result.stdout.strip().split(\"\\n\"):\n",
    "            line = line.strip()\n",
    "            if line and line.isdigit():\n",
    "                pid = int(line)\n",
    "                if pid != my_pid:\n",
    "                    try:\n",
    "                        os.kill(pid, signal.SIGKILL)\n",
    "                        killed.append(pid)\n",
    "                    except ProcessLookupError:\n",
    "                        pass\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: {e}\")\n",
    "\n",
    "    # Also clear PyTorch cache if available\n",
    "    try:\n",
    "        import torch\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    if killed:\n",
    "        print(f\"Killed {len(killed)} leftover GPU processes: {killed}\")\n",
    "    else:\n",
    "        print(\"No leftover GPU processes found.\")\n",
    "\n",
    "    # Show clean state\n",
    "    os.system(\"nvidia-smi --query-gpu=index,memory.used,memory.total --format=csv\")\n",
    "\n",
    "nuke_gpu_processes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate bitsandbytes huggingface_hub sentencepiece protobuf -q\n",
    "!pip install vllm -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Log in to HuggingFace\n",
    "\n",
    "HuggingFace is the largest hub for open-source AI models.  \n",
    "To download models, you need a free account and an access token.\n",
    "\n",
    "**How to get your token:**\n",
    "1. Go to https://huggingface.co and create a free account\n",
    "2. Go to https://huggingface.co/settings/tokens\n",
    "3. Click **New token** -- name it anything -- select **Read** access -- click **Create**\n",
    "4. Copy the token (starts with `hf_`) and paste it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in to HuggingFace!\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Paste your HuggingFace token here:\n",
    "HF_TOKEN = \"YOUR_HUGGINGFACE_TOKEN\"\n",
    "\n",
    "login(token=HF_TOKEN)\n",
    "print(\"Logged in to HuggingFace!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Download the model from HuggingFace\n",
    "\n",
    "We are downloading **Qwen2.5 72B Instruct** -- a 72 billion parameter model from Alibaba.  \n",
    "The files total ~136GB. This will take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: Qwen/Qwen2.5-72B-Instruct\n",
      "72 billion parameters = ~136GB of files\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996558059aa44082b9a88e87f621402f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 47 files:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download complete!\n",
      "Saved to: /workspace/models/models--Qwen--Qwen2.5-72B-Instruct/snapshots/495f39366efef23836d0cfae4fbe635880d2be31\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "\n",
    "print(f\"Downloading: {MODEL_NAME}\")\n",
    "print(f\"72 billion parameters = ~136GB of files\")\n",
    "print()\n",
    "\n",
    "model_path = snapshot_download(MODEL_NAME, cache_dir=\"/workspace/models\")\n",
    "\n",
    "print(f\"\\nDownload complete!\")\n",
    "print(f\"Saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: The Problem -- This Model Doesn't Fit\n",
    "\n",
    "Let's check our hardware and do the math."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Check our GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs available: 2\n",
      "\n",
      "  GPU 0 (NVIDIA A100-SXM4-80GB): 0.0 GB used / 85 GB total\n",
      "  GPU 1 (NVIDIA A100-SXM4-80GB): 0.0 GB used / 85 GB total\n",
      "\n",
      "Both GPUs are empty. Let's try to load the model.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def gpu_status():\n",
    "    \"\"\"Print GPU memory usage.\"\"\"\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        total = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "        used = torch.cuda.memory_allocated(i) / 1e9\n",
    "        print(f\"  GPU {i} ({torch.cuda.get_device_name(i)}): {used:.1f} GB used / {total:.0f} GB total\")\n",
    "\n",
    "print(f\"GPUs available: {torch.cuda.device_count()}\")\n",
    "print()\n",
    "gpu_status()\n",
    "print()\n",
    "print(\"Both GPUs are empty. Let's try to load the model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Do the math\n",
    "\n",
    "The model has **72 billion parameters**.  \n",
    "Each parameter is stored as a 16-bit float (FP16) = **2 bytes**.  \n",
    "\n",
    "```\n",
    "72,000,000,000 params x 2 bytes = 144,000,000,000 bytes = 144 GB\n",
    "```\n",
    "\n",
    "One A100 GPU has **80 GB** of VRAM.  \n",
    "\n",
    "**144 GB > 80 GB. The model does not fit on one GPU.**\n",
    "\n",
    "But let's try anyway..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Try to load on 1 GPU -- watch it crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import gc\n",
    "\n",
    "print(\"Attempting to load 144GB model onto 1 GPU (80GB)...\")\n",
    "print(\"This WILL fail.\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map={\"\":  \"cuda:0\"},  # Force everything onto GPU 0\n",
    "    )\n",
    "except Exception as e:\n",
    "    # CRITICAL: Clean up the partial allocation from the failed load!\n",
    "    # Without this, device_map=\"auto\" will see less free VRAM and\n",
    "    # offload layers to CPU, making inference ~20x slower.\n",
    "    try:\n",
    "        del model\n",
    "    except NameError:\n",
    "        pass\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"CRASHED: {type(e).__name__}\")\n",
    "    print(f\"\\n{e}\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"The model is 144GB. The GPU has 80GB. It doesn't fit.\")\n",
    "    print(\"We need a solution.\")\n",
    "    print()\n",
    "    gpu_status()  # Verify GPUs are clean after cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 So what do we do?\n",
    "\n",
    "We have 3 main strategies:\n",
    "\n",
    "1. **Pipeline Parallelism** -- Split the model's layers across multiple GPUs. GPU 0 gets the first half of layers, GPU 1 gets the second half. Data flows through like an assembly line.\n",
    "\n",
    "2. **Quantization** -- Use smaller numbers (4-bit instead of 16-bit) so the model shrinks to fit on 1 GPU.\n",
    "\n",
    "3. **Tensor Parallelism** -- Split individual weight matrices across GPUs. Each GPU computes a partial result, then they combine. This is what production systems use.\n",
    "\n",
    "**Our plan:**\n",
    "- First, we'll try Pipeline Parallelism and Quantization using **HuggingFace** (easy to understand)\n",
    "- Then, we'll learn how Tensor Parallelism works under the hood\n",
    "- Finally, we'll use **vLLM** to do a **fair speed comparison** between Tensor and Pipeline Parallelism (same engine, different strategies)\n",
    "\n",
    "Let's start!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Pipeline Parallelism with HuggingFace (2 GPUs)\n",
    "\n",
    "**Idea**: Don't split any individual matrix. Instead, put some layers on GPU 0 and other layers on GPU 1.  \n",
    "Data flows from GPU 0 to GPU 1, like an assembly line.\n",
    "\n",
    "```\n",
    "Input --> [GPU 0: Layers 0-39] --> transfer --> [GPU 1: Layers 40-79] --> Output\n",
    "```\n",
    "\n",
    "HuggingFace does this automatically with `device_map=\"auto\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load with pipeline parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 70B model with device_map='auto' (pipeline parallelism)...\n",
      "This splits LAYERS across GPUs.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65d9e2a51814b86ae375ea6a5c94799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "CPU times: user 7min 24s, sys: 6min 36s, total: 14min\n",
      "Wall time: 48.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\"Loading 70B model with device_map='auto' (pipeline parallelism)...\")\n",
    "print(\"This splits LAYERS across GPUs.\")\n",
    "print()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Proof: both GPUs are loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb 24 10:35:40 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:BD:00.0 Off |                    0 |\n",
      "| N/A   28C    P0             82W /  500W |   68087MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:CD:00.0 Off |                    0 |\n",
      "| N/A   28C    P0             72W /  500W |   71439MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Which layers are on which GPU?\n",
    "\n",
    "This is pipeline parallelism: the first half of layers on GPU 0, second half on GPU 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE PARALLELISM: Layer-to-GPU Mapping\n",
      "==================================================\n",
      "GPU 0: Layers 0-38 (39 layers)\n",
      "GPU 1: Layers 39-79 (41 layers)\n",
      "\n",
      "Data flows: Input -> GPU 0 -> GPU 1 -> Output\n",
      "This is an assembly line: each GPU handles a different stage.\n"
     ]
    }
   ],
   "source": [
    "# Show the layer-to-GPU mapping\n",
    "gpu0_layers = []\n",
    "gpu1_layers = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"layers.\" in name:\n",
    "        layer_num = int(name.split(\"layers.\")[1].split(\".\")[0])\n",
    "        device = str(param.device)\n",
    "        if \"cuda:0\" in device:\n",
    "            if layer_num not in gpu0_layers:\n",
    "                gpu0_layers.append(layer_num)\n",
    "        elif \"cuda:1\" in device:\n",
    "            if layer_num not in gpu1_layers:\n",
    "                gpu1_layers.append(layer_num)\n",
    "\n",
    "gpu0_layers.sort()\n",
    "gpu1_layers.sort()\n",
    "\n",
    "print(\"PIPELINE PARALLELISM: Layer-to-GPU Mapping\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"GPU 0: Layers {gpu0_layers[0]}-{gpu0_layers[-1]} ({len(gpu0_layers)} layers)\")\n",
    "print(f\"GPU 1: Layers {gpu1_layers[0]}-{gpu1_layers[-1]} ({len(gpu1_layers)} layers)\")\n",
    "print()\n",
    "print(\"Data flows: Input -> GPU 0 -> GPU 1 -> Output\")\n",
    "print(\"This is an assembly line: each GPU handles a different stage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Generate text and measure speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'A farmer has 3 fields. Field A produces twice as much wheat as Field B. Field C produces 50kg more than Field A. Together all three fields produce 550kg. How much does each field produce?'\n",
      "\n",
      "Generating with Pipeline Parallelism...\n",
      "\n",
      "A farmer has 3 fields. Field A produces twice as much wheat as Field B. Field C produces 50kg more than Field A. Together all three fields produce 550kg. How much does each field produce? Let's denote the amount of wheat produced by Field B as \\( x \\) kg.\n",
      "\n",
      "Given:\n",
      "- Field A produces twice as much wheat as Field B, so Field A produces \\( 2x \\) kg.\n",
      "- Field C produces 50 kg more than Field A, so Field C produces \\( 2x + 50 \\) kg.\n",
      "- The total production from all three fields is 550 kg.\n",
      "\n",
      "We can set up the following equation to represent the total production:\n",
      "\n",
      "\\[\n",
      "x + 2x + (2x + 50) = 550\n",
      "\\]\n",
      "\n",
      "Simplify the equation:\n",
      "\n",
      "\\[\n",
      "x + 2x + 2x + 50 = 550\n",
      "\\]\n",
      "\n",
      "Combine like terms:\n",
      "\n",
      "\\[\n",
      "5x + 50 = 550\n",
      "\\]\n",
      "\n",
      "Subtract 50 from both sides:\n",
      "\n",
      "\\[\n",
      "5x = 500\n",
      "\\]\n",
      "\n",
      "Divide both sides by 5:\n",
      "\n",
      "\\[\n",
      "x = 100\n",
      "\\]\n",
      "\n",
      "Now we can find the production for each field:\n",
      "- Field B produces \\( x = 100 \\) kg.\n",
      "- Field A produces \\( 2x = 2 \\times 100 = 200 \\) kg.\n",
      "- Field C produces \\( 2x + 50 = 200 + 50 = 250 \\) kg.\n",
      "\n",
      "So, the production for each field is:\n",
      "- Field A: 200 kg\n",
      "- Field B: 100 kg\n",
      "- Field C: 250 kg\n",
      "\n",
      "To verify, we can add the amounts together:\n",
      "\n",
      "\\[\n",
      "100 + 200 + 250 = 550 \\text{ kg}\n",
      "\\]\n",
      "\n",
      "The solution is correct. Each field produces:\n",
      "- Field A: 200 kg\n",
      "- Field B: 100 kg\n",
      "- Field C: 250 kg\n",
      "\n",
      "============================================================\n",
      "PIPELINE PARALLELISM RESULTS (HuggingFace, 2 GPUs):\n",
      "  Total time:                 47.0s\n",
      "  Tokens generated:           409\n",
      "  Speed:                      8.7 tokens/sec\n",
      "\n",
      "Correct answer: Field B = 100kg, Field A = 200kg, Field C = 250kg\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# A reasoning question with a clear, concise answer\n",
    "prompt = \"A farmer has 3 fields. Field A produces twice as much wheat as Field B. Field C produces 50kg more than Field A. Together all three fields produce 550kg. How much does each field produce?\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "num_input_tokens = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"\\nGenerating with Pipeline Parallelism...\\n\")\n",
    "\n",
    "# Custom streamer that also tracks time-to-first-token\n",
    "class TimedStreamer(TextStreamer):\n",
    "    def __init__(self, tokenizer, **kwargs):\n",
    "        super().__init__(tokenizer, **kwargs)\n",
    "        self.start_time = time.time()\n",
    "        self.first_token_time = None\n",
    "\n",
    "    def on_finalized_text(self, text, stream_end=False):\n",
    "        if self.first_token_time is None and text:\n",
    "            self.first_token_time = time.time()\n",
    "        print(text, end=\"\", flush=True)\n",
    "\n",
    "streamer = TimedStreamer(tokenizer, skip_special_tokens=True)\n",
    "\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=2048,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        streamer=streamer,\n",
    "    )\n",
    "pp_total_time = time.time() - start\n",
    "\n",
    "pp_tokens = outputs.shape[1] - num_input_tokens\n",
    "pp_ttft = streamer.first_token_time - start if streamer.first_token_time else pp_total_time\n",
    "pp_speed = pp_tokens / pp_total_time\n",
    "\n",
    "print(f\"\\n\\n{'='*60}\")\n",
    "print(f\"PIPELINE PARALLELISM RESULTS (HuggingFace, 2 GPUs):\")\n",
    "print(f\"  Total time:                 {pp_total_time:.1f}s\")\n",
    "print(f\"  Tokens generated:           {pp_tokens}\")\n",
    "print(f\"  Speed:                      {pp_speed:.1f} tokens/sec\")\n",
    "print(f\"\\nCorrect answer: Field B = 100kg, Field A = 200kg, Field C = 250kg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Quantization with HuggingFace (1 GPU)\n",
    "\n",
    "What if instead of splitting the model across GPUs, we **shrink** it?\n",
    "\n",
    "Each weight is stored as a 16-bit float (FP16 = 2 bytes).  \n",
    "What if we used 4-bit integers instead (INT4 = 0.5 bytes)?\n",
    "\n",
    "```\n",
    "FP16: 70B x 2 bytes   = 140 GB --> needs 2 GPUs\n",
    "INT4: 70B x 0.5 bytes = 35 GB  --> fits on 1 GPU!\n",
    "```\n",
    "\n",
    "The tradeoff: less precision means slightly lower quality.  \n",
    "Like compressing a photo from RAW to JPEG -- smaller file, almost the same picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 First, unload the FP16 model\n",
    "\n",
    "We need to free GPU memory before loading the quantized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP16 model unloaded. GPU memory freed.\n",
      "  GPU 0 (NVIDIA A100-SXM4-80GB): 0.0 GB used / 85 GB total\n",
      "  GPU 1 (NVIDIA A100-SXM4-80GB): 2.5 GB used / 85 GB total\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"FP16 model unloaded. GPU memory freed.\")\n",
    "gpu_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load the same model in 4-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the SAME 70B model in 4-bit quantization...\n",
      "70B x 0.5 bytes = ~35GB. This should fit on a SINGLE GPU.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24d2b6131f54f1084da748840e5343b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4-bit model loaded!\n",
      "CPU times: user 7min 28s, sys: 6min 55s, total: 14min 24s\n",
      "Wall time: 53.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "print(\"Loading the SAME 70B model in 4-bit quantization...\")\n",
    "print(\"70B x 0.5 bytes = ~35GB. This should fit on a SINGLE GPU.\")\n",
    "print()\n",
    "\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"\\n4-bit model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Proof: it fits on 1 GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb 24 10:37:21 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:BD:00.0 Off |                    0 |\n",
      "| N/A   28C    P0             82W /  500W |   29377MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:CD:00.0 Off |                    0 |\n",
      "| N/A   28C    P0             77W /  500W |   41465MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to Part 2 where we needed **both** GPUs.  \n",
    "Now the same 70B model fits on **one** GPU thanks to quantization.\n",
    "\n",
    "### 3.4 Generate text -- does quality hold up?\n",
    "\n",
    "Same prompt, same token count. Let's see if the quantized model can still reason correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'A farmer has 3 fields. Field A produces twice as much wheat as Field B. Field C produces 50kg more than Field A. Together all three fields produce 550kg. How much does each field produce?'\n",
      "\n",
      "Generating with 4-bit Quantization (1 GPU)...\n",
      "\n",
      "A farmer has 3 fields. Field A produces twice as much wheat as Field B. Field C produces 50kg more than Field A. Together all three fields produce 550kg. How much does each field produce? Let's denote the amount of wheat produced by Field B as \\( x \\) kg.\n",
      "\n",
      "According to the problem:\n",
      "- Field A produces twice as much wheat as Field B, so Field A produces \\( 2x \\) kg.\n",
      "- Field C produces 50 kg more than Field A, so Field C produces \\( 2x + 50 \\) kg.\n",
      "\n",
      "The total production from all three fields is 550 kg. Therefore, we can set up the following equation:\n",
      "\n",
      "\\[\n",
      "x + 2x + (2x + 50) = 550\n",
      "\\]\n",
      "\n",
      "Simplify the equation:\n",
      "\n",
      "\\[\n",
      "x + 2x + 2x + 50 = 550\n",
      "\\]\n",
      "\n",
      "Combine like terms:\n",
      "\n",
      "\\[\n",
      "5x + 50 = 550\n",
      "\\]\n",
      "\n",
      "Subtract 50 from both sides:\n",
      "\n",
      "\\[\n",
      "5x = 500\n",
      "\\]\n",
      "\n",
      "Divide both sides by 5:\n",
      "\n",
      "\\[\n",
      "x = 100\n",
      "\\]\n",
      "\n",
      "Now we can find the production for each field:\n",
      "- Field B produces \\( x = 100 \\) kg.\n",
      "- Field A produces \\( 2x = 2 \\times 100 = 200 \\) kg.\n",
      "- Field C produces \\( 2x + 50 = 200 + 50 = 250 \\) kg.\n",
      "\n",
      "So, the production from each field is:\n",
      "- Field A: 200 kg\n",
      "- Field B: 100 kg\n",
      "- Field C: 250 kg\n",
      "\n",
      "To verify, we can check the total production:\n",
      "\n",
      "\\[\n",
      "100 + 200 + 250 = 550 \\text{ kg}\n",
      "\\]\n",
      "\n",
      "The solution is correct. Each field produces:\n",
      "- Field A: 200 kg\n",
      "- Field B: 100 kg\n",
      "- Field C: 250 kg\n",
      "\n",
      "============================================================\n",
      "QUANTIZED INT4 RESULTS (HuggingFace, 1 GPU):\n",
      "  Total time:                 33.2s\n",
      "  Tokens generated:           408\n",
      "  Speed:                      12.3 tokens/sec\n",
      "\n",
      "Correct answer: Field B = 100kg, Field A = 200kg, Field C = 250kg\n",
      "Compare the reasoning quality to the FP16 version above.\n",
      "\n",
      "HuggingFace COMPARISON:\n",
      "  Pipeline Parallel (2 GPUs, FP16):     8.7 tok/s  (409 tokens in 47.0s)\n",
      "  Quantized INT4    (1 GPU,  INT4):    12.3 tok/s  (408 tokens in 33.2s)\n"
     ]
    }
   ],
   "source": [
    "# Re-define TimedStreamer (in case of kernel restart)\n",
    "class TimedStreamer(TextStreamer):\n",
    "    def __init__(self, tokenizer, **kwargs):\n",
    "        super().__init__(tokenizer, **kwargs)\n",
    "        self.start_time = time.time()\n",
    "        self.first_token_time = None\n",
    "\n",
    "    def on_finalized_text(self, text, stream_end=False):\n",
    "        if self.first_token_time is None and text:\n",
    "            self.first_token_time = time.time()\n",
    "        print(text, end=\"\", flush=True)\n",
    "\n",
    "# Same prompt as Pipeline Parallelism\n",
    "prompt = \"A farmer has 3 fields. Field A produces twice as much wheat as Field B. Field C produces 50kg more than Field A. Together all three fields produce 550kg. How much does each field produce?\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_4bit.device)\n",
    "num_input_tokens = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"\\nGenerating with 4-bit Quantization (1 GPU)...\\n\")\n",
    "\n",
    "streamer = TimedStreamer(tokenizer, skip_special_tokens=True)\n",
    "\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    outputs = model_4bit.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=2048,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        streamer=streamer,\n",
    "    )\n",
    "q_total_time = time.time() - start\n",
    "\n",
    "q_tokens = outputs.shape[1] - num_input_tokens\n",
    "q_speed = q_tokens / q_total_time\n",
    "\n",
    "print(f\"\\n\\n{'='*60}\")\n",
    "print(f\"QUANTIZED INT4 RESULTS (HuggingFace, 1 GPU):\")\n",
    "print(f\"  Total time:                 {q_total_time:.1f}s\")\n",
    "print(f\"  Tokens generated:           {q_tokens}\")\n",
    "print(f\"  Speed:                      {q_speed:.1f} tokens/sec\")\n",
    "print(f\"\\nCorrect answer: Field B = 100kg, Field A = 200kg, Field C = 250kg\")\n",
    "print(f\"Compare the reasoning quality to the FP16 version above.\")\n",
    "print()\n",
    "print(f\"HuggingFace COMPARISON:\")\n",
    "print(f\"  Pipeline Parallel (2 GPUs, FP16):  {pp_speed:>6.1f} tok/s  ({pp_tokens} tokens in {pp_total_time:.1f}s)\")\n",
    "print(f\"  Quantized INT4    (1 GPU,  INT4):  {q_speed:>6.1f} tok/s  ({q_tokens} tokens in {q_total_time:.1f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Save HuggingFace results and restart kernel\n",
    "\n",
    "We need to fully release GPU memory before loading vLLM.  \n",
    "The safest way: **save our results, restart the kernel**, then continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFace results saved to /workspace/hf_results.json\n",
      "  Pipeline Parallel: 8.7 tok/s | 409 tokens in 47.0s\n",
      "  Quantized INT4:    12.3 tok/s | 408 tokens in 33.2s\n",
      "\n",
      "============================================================\n",
      "  NOW: Restart the kernel (Kernel -> Restart)\n",
      "  Then continue from Part 4 below\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Save all HuggingFace results so we can compare later\n",
    "hf_results = {\n",
    "    \"pp_speed\": pp_speed,\n",
    "    \"pp_ttft\": pp_ttft,\n",
    "    \"pp_total_time\": pp_total_time,\n",
    "    \"pp_tokens\": pp_tokens,\n",
    "    \"q_speed\": q_speed,\n",
    "    \"q_total_time\": q_total_time,\n",
    "    \"q_tokens\": q_tokens,\n",
    "    \"model_path\": model_path,\n",
    "}\n",
    "with open(\"/workspace/hf_results.json\", \"w\") as f:\n",
    "    json.dump(hf_results, f)\n",
    "\n",
    "print(\"HuggingFace results saved to /workspace/hf_results.json\")\n",
    "print(f\"  Pipeline Parallel: {pp_speed:.1f} tok/s | {pp_tokens} tokens in {pp_total_time:.1f}s\")\n",
    "print(f\"  Quantized INT4:    {q_speed:.1f} tok/s | {q_tokens} tokens in {q_total_time:.1f}s\")\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"  NOW: Restart the kernel (Kernel -> Restart)\")\n",
    "print(\"  Then continue from Part 4 below\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Understanding Tensor Parallelism (Concept)\n",
    "\n",
    "Pipeline parallelism splits **layers** across GPUs. But there's another approach:  \n",
    "**Tensor parallelism splits individual weight matrices.**\n",
    "\n",
    "Instead of giving each GPU complete layers, we give each GPU **half of every matrix**.  \n",
    "Each GPU computes a partial result, then they combine.\n",
    "\n",
    "```\n",
    "PIPELINE PARALLELISM                    TENSOR PARALLELISM\n",
    "(each GPU has COMPLETE layers)          (each GPU has HALF of EVERY layer)\n",
    "\n",
    "┌─────────────┐                         ┌──────────┬──────────┐\n",
    "│   GPU 0     │                         │  GPU 0   │  GPU 1   │\n",
    "│             │                         │          │          │\n",
    "│  Layer 0    │                         │ Layer 0  │ Layer 0  │\n",
    "│  Layer 1    │                         │ (left)   │ (right)  │\n",
    "│  ...        │                         │          │          │\n",
    "│  Layer 39   │                         │ Layer 1  │ Layer 1  │\n",
    "├─────────────┤  data transfer          │ (left)   │ (right)  │\n",
    "│   GPU 1     │  ↓ between GPUs         │          │          │\n",
    "│             │                         │ ...      │ ...      │\n",
    "│  Layer 40   │                         │          │          │\n",
    "│  Layer 41   │                         │ Layer 79 │ Layer 79 │\n",
    "│  ...        │                         │ (left)   │ (right)  │\n",
    "│  Layer 79   │                         │          │          │\n",
    "└─────────────┘                         └──────────┴──────────┘\n",
    "Sequential flow                         Parallel computation\n",
    "1 big transfer between GPUs             Many small transfers (AllReduce)\n",
    "```\n",
    "\n",
    "**Why is TP faster?** Pipeline parallelism has **pipeline bubbles**: GPU 1 sits idle while GPU 0 processes, then vice versa. Tensor parallelism keeps **both GPUs busy simultaneously** on every layer.\n",
    "\n",
    "The tradeoff: TP requires **fast GPU-to-GPU communication** (NVLink at 600+ GB/s). PP only needs occasional transfers. This is why:\n",
    "- **TP is used within a server** (GPUs connected by NVLink)\n",
    "- **PP is used across servers** (connected by slower InfiniBand)\n",
    "\n",
    "We'll see the speed difference in Part 6 using vLLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Data Parallelism (Training Concept)\n",
    "\n",
    "We've seen two ways to split a model across GPUs:  \n",
    "- **Pipeline Parallelism**: split layers  \n",
    "- **Tensor Parallelism**: split matrices  \n",
    "\n",
    "But there's a third type: **Data Parallelism**.\n",
    "\n",
    "Data parallelism is primarily a **training** technique:  \n",
    "- Make **complete copies** of the model on separate GPUs  \n",
    "- Split the **training batch** across copies -- each GPU processes different data  \n",
    "- After each step, **synchronize gradients** across all copies (AllReduce)  \n",
    "- Every copy stays in sync and learns from all the data  \n",
    "\n",
    "```\n",
    "TRAINING with Data Parallelism:\n",
    "\n",
    "  Training Batch (1024 samples)\n",
    "       |\n",
    "       ├──→ GPU 0: Model copy 1 (256 samples) ──→ gradients ──┐\n",
    "       ├──→ GPU 1: Model copy 2 (256 samples) ──→ gradients ──┤ AllReduce\n",
    "       ├──→ GPU 2: Model copy 3 (256 samples) ──→ gradients ──┤ (average)\n",
    "       └──→ GPU 3: Model copy 4 (256 samples) ──→ gradients ──┘\n",
    "                                                                │\n",
    "                                                   All GPUs update weights\n",
    "                                                   with averaged gradients\n",
    "```\n",
    "\n",
    "Data parallelism doesn't help fit a bigger model -- each GPU needs the full model.  \n",
    "It helps **train faster** by processing more data in parallel.\n",
    "\n",
    "For **inference**, a similar idea is used (multiple model replicas behind a load balancer),  \n",
    "but it's simpler -- no gradient sync needed, each replica is fully independent.  \n",
    "\n",
    "We can't demo this on 2 GPUs (the model already takes both GPUs),  \n",
    "but this is how large-scale training works in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Real-world training at scale: 3D Parallelism\n",
    "\n",
    "How would you **train** a 744B parameter model (like GLM-5)?  \n",
    "You need all 3 types of parallelism working together:\n",
    "\n",
    "```\n",
    "Model size: 744B params x 2 bytes = 1,488 GB (~1.5 TB)\n",
    "One H100 GPU: 80 GB\n",
    "\n",
    "Step 1 - Tensor Parallelism (within each server):\n",
    "  8 GPUs per server, connected by NVLink (900 GB/s)\n",
    "  Each GPU holds 1/8th of every weight matrix\n",
    "  TP = 8\n",
    "\n",
    "Step 2 - Pipeline Parallelism (across servers):\n",
    "  4 servers, connected by InfiniBand (50 GB/s)\n",
    "  Each server handles ~20 layers\n",
    "  PP = 4\n",
    "\n",
    "Step 3 - Data Parallelism (for throughput):\n",
    "  33 complete copies of the 4-server setup\n",
    "  Each copy trains on different data, then syncs gradients\n",
    "  DP = 33\n",
    "\n",
    "Total: TP(8) x PP(4) x DP(33) = 1,056 H100 GPUs\n",
    "Cost: ~$30M in GPU hardware alone\n",
    "```\n",
    "\n",
    "This is called **3D parallelism** -- combining all three types.  \n",
    "Frameworks like Megatron-LM and DeepSpeed make this possible.\n",
    "\n",
    "Now let's see a **fair speed comparison** using vLLM: Tensor Parallel vs Pipeline Parallel, same engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Fair Speed Comparison with vLLM\n",
    "\n",
    "Earlier we used HuggingFace to demonstrate Pipeline Parallelism and Quantization.  \n",
    "But HuggingFace's `model.generate()` is simple and unoptimized.\n",
    "\n",
    "**vLLM** is a production inference engine with PagedAttention, fused kernels, and other optimizations.  \n",
    "It supports both **Tensor Parallelism** and **Pipeline Parallelism** -- so we can compare them fairly.\n",
    "\n",
    "Same engine, same model, different parallelism strategy. **Apple-to-apple comparison.**\n",
    "\n",
    "> **If you restarted the kernel**, start running from here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Load HuggingFace results + verify GPUs are clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace results:\n",
      "  Pipeline Parallel (HF): 8.7 tok/s | 409 tokens in 47.0s\n",
      "  Quantized INT4    (HF): 12.3 tok/s | 408 tokens in 33.2s\n",
      "\n",
      "  GPU 0 (NVIDIA A100-SXM4-80GB): 0.0 GB used / 85 GB total\n",
      "  GPU 1 (NVIDIA A100-SXM4-80GB): 0.0 GB used / 85 GB total\n",
      "\n",
      "GPUs are clean. Ready for vLLM.\n"
     ]
    }
   ],
   "source": [
    "import torch, time, json\n",
    "\n",
    "# Reload HuggingFace results from disk\n",
    "with open(\"/workspace/hf_results.json\", \"r\") as f:\n",
    "    hf = json.load(f)\n",
    "\n",
    "pp_speed = hf[\"pp_speed\"]\n",
    "pp_total_time = hf[\"pp_total_time\"]\n",
    "pp_tokens = hf[\"pp_tokens\"]\n",
    "q_speed = hf[\"q_speed\"]\n",
    "q_total_time = hf[\"q_total_time\"]\n",
    "q_tokens = hf[\"q_tokens\"]\n",
    "model_path = hf[\"model_path\"]\n",
    "\n",
    "print(f\"Loaded HuggingFace results:\")\n",
    "print(f\"  Pipeline Parallel (HF): {pp_speed:.1f} tok/s | {pp_tokens} tokens in {pp_total_time:.1f}s\")\n",
    "print(f\"  Quantized INT4    (HF): {q_speed:.1f} tok/s | {q_tokens} tokens in {q_total_time:.1f}s\")\n",
    "print()\n",
    "\n",
    "# Verify GPUs are clean\n",
    "def gpu_status():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        total = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "        used = torch.cuda.memory_allocated(i) / 1e9\n",
    "        print(f\"  GPU {i} ({torch.cuda.get_device_name(i)}): {used:.1f} GB used / {total:.0f} GB total\")\n",
    "\n",
    "gpu_status()\n",
    "print(\"\\nGPUs are clean. Ready for vLLM.\")\n",
    "\n",
    "prompt = \"A farmer has 3 fields. Field A produces twice as much wheat as Field B. Field C produces 50kg more than Field A. Together all three fields produce 550kg. How much does each field produce?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 vLLM Tensor Parallel (2 GPUs)\n",
    "\n",
    "**Tensor Parallelism**: split every weight matrix across GPUs. Both GPUs compute in parallel on every layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 70B model with vLLM Tensor Parallelism...\n",
      "tensor_parallel_size=2\n",
      "This splits MATRICES across GPUs -- both GPUs work on every layer.\n",
      "\n",
      "INFO 02-24 10:38:46 [utils.py:261] non-default args: {'dtype': 'float16', 'max_model_len': 2048, 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.92, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'model': '/workspace/models/models--Qwen--Qwen2.5-72B-Instruct/snapshots/495f39366efef23836d0cfae4fbe635880d2be31'}\n",
      "INFO 02-24 10:38:54 [model.py:541] Resolved architecture: Qwen2ForCausalLM\n",
      "WARNING 02-24 10:38:54 [model.py:1885] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 02-24 10:38:54 [model.py:1561] Using max model len 2048\n",
      "INFO 02-24 10:38:54 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 02-24 10:38:54 [vllm.py:624] Asynchronous scheduling is enabled.\n",
      "WARNING 02-24 10:38:54 [vllm.py:662] Enforce eager set, overriding optimization level to -O0\n",
      "INFO 02-24 10:38:54 [vllm.py:762] Cudagraph is disabled under eager mode\n",
      "WARNING 02-24 10:38:55 [system_utils.py:140] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3339)\u001b[0;0m INFO 02-24 10:39:01 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/workspace/models/models--Qwen--Qwen2.5-72B-Instruct/snapshots/495f39366efef23836d0cfae4fbe635880d2be31', speculative_config=None, tokenizer='/workspace/models/models--Qwen--Qwen2.5-72B-Instruct/snapshots/495f39366efef23836d0cfae4fbe635880d2be31', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/workspace/models/models--Qwen--Qwen2.5-72B-Instruct/snapshots/495f39366efef23836d0cfae4fbe635880d2be31, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3339)\u001b[0;0m WARNING 02-24 10:39:01 [multiproc_executor.py:910] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 02-24 10:39:08 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:59321 backend=nccl\n",
      "INFO 02-24 10:39:08 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:59321 backend=nccl\n",
      "INFO 02-24 10:39:08 [pynccl.py:111] vLLM is using nccl==2.27.5\n",
      "WARNING 02-24 10:39:08 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.\n",
      "WARNING 02-24 10:39:08 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.\n",
      "INFO 02-24 10:39:08 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A\n",
      "INFO 02-24 10:39:08 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A\n",
      "\u001b[0;36m(Worker_TP0 pid=3537)\u001b[0;0m INFO 02-24 10:39:10 [gpu_model_runner.py:4033] Starting to load model /workspace/models/models--Qwen--Qwen2.5-72B-Instruct/snapshots/495f39366efef23836d0cfae4fbe635880d2be31...\n",
      "\u001b[0;36m(Worker_TP0 pid=3537)\u001b[0;0m INFO 02-24 10:39:25 [cuda.py:364] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/37 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   3% Completed | 1/37 [00:01<00:54,  1.51s/it]\n",
      "Loading safetensors checkpoint shards:   5% Completed | 2/37 [00:03<00:57,  1.65s/it]\n",
      "Loading safetensors checkpoint shards:   8% Completed | 3/37 [00:04<00:56,  1.67s/it]\n",
      "Loading safetensors checkpoint shards:  11% Completed | 4/37 [00:06<00:57,  1.73s/it]\n",
      "Loading safetensors checkpoint shards:  14% Completed | 5/37 [00:08<00:55,  1.74s/it]\n",
      "Loading safetensors checkpoint shards:  16% Completed | 6/37 [00:10<00:54,  1.76s/it]\n",
      "Loading safetensors checkpoint shards:  19% Completed | 7/37 [00:12<00:52,  1.74s/it]\n",
      "Loading safetensors checkpoint shards:  22% Completed | 8/37 [00:13<00:51,  1.77s/it]\n",
      "Loading safetensors checkpoint shards:  24% Completed | 9/37 [00:15<00:49,  1.76s/it]\n",
      "Loading safetensors checkpoint shards:  27% Completed | 10/37 [00:17<00:47,  1.75s/it]\n",
      "Loading safetensors checkpoint shards:  30% Completed | 11/37 [00:19<00:45,  1.73s/it]\n",
      "Loading safetensors checkpoint shards:  32% Completed | 12/37 [00:20<00:43,  1.76s/it]\n",
      "Loading safetensors checkpoint shards:  35% Completed | 13/37 [00:22<00:42,  1.77s/it]\n",
      "Loading safetensors checkpoint shards:  38% Completed | 14/37 [00:24<00:41,  1.78s/it]\n",
      "Loading safetensors checkpoint shards:  41% Completed | 15/37 [00:26<00:38,  1.77s/it]\n",
      "Loading safetensors checkpoint shards:  43% Completed | 16/37 [00:27<00:37,  1.78s/it]\n",
      "Loading safetensors checkpoint shards:  46% Completed | 17/37 [00:29<00:35,  1.78s/it]\n",
      "Loading safetensors checkpoint shards:  49% Completed | 18/37 [00:31<00:33,  1.77s/it]\n",
      "Loading safetensors checkpoint shards:  51% Completed | 19/37 [00:33<00:31,  1.75s/it]\n",
      "Loading safetensors checkpoint shards:  54% Completed | 20/37 [00:35<00:30,  1.77s/it]\n",
      "Loading safetensors checkpoint shards:  57% Completed | 21/37 [00:36<00:28,  1.79s/it]\n",
      "Loading safetensors checkpoint shards:  59% Completed | 22/37 [00:38<00:26,  1.78s/it]\n",
      "Loading safetensors checkpoint shards:  62% Completed | 23/37 [00:40<00:25,  1.80s/it]\n",
      "Loading safetensors checkpoint shards:  65% Completed | 24/37 [00:42<00:23,  1.79s/it]\n",
      "Loading safetensors checkpoint shards:  68% Completed | 25/37 [00:44<00:21,  1.78s/it]\n",
      "Loading safetensors checkpoint shards:  70% Completed | 26/37 [00:45<00:19,  1.78s/it]\n",
      "Loading safetensors checkpoint shards:  73% Completed | 27/37 [00:47<00:17,  1.78s/it]\n",
      "Loading safetensors checkpoint shards:  76% Completed | 28/37 [00:49<00:16,  1.79s/it]\n",
      "Loading safetensors checkpoint shards:  78% Completed | 29/37 [00:51<00:14,  1.79s/it]\n",
      "Loading safetensors checkpoint shards:  81% Completed | 30/37 [00:52<00:12,  1.79s/it]\n",
      "Loading safetensors checkpoint shards:  84% Completed | 31/37 [00:54<00:10,  1.77s/it]\n",
      "Loading safetensors checkpoint shards:  86% Completed | 32/37 [00:56<00:09,  1.80s/it]\n",
      "Loading safetensors checkpoint shards:  89% Completed | 33/37 [00:58<00:07,  1.80s/it]\n",
      "Loading safetensors checkpoint shards:  92% Completed | 34/37 [01:00<00:05,  1.81s/it]\n",
      "Loading safetensors checkpoint shards:  95% Completed | 35/37 [01:01<00:03,  1.78s/it]\n",
      "Loading safetensors checkpoint shards:  97% Completed | 36/37 [01:03<00:01,  1.78s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 37/37 [01:05<00:00,  1.75s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 37/37 [01:05<00:00,  1.77s/it]\n",
      "\u001b[0;36m(Worker_TP0 pid=3537)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(Worker_TP0 pid=3537)\u001b[0;0m INFO 02-24 10:40:32 [default_loader.py:291] Loading weights took 65.44 seconds\n",
      "\u001b[0;36m(Worker_TP0 pid=3537)\u001b[0;0m INFO 02-24 10:40:33 [gpu_model_runner.py:4130] Model loading took 67.8 GiB memory and 82.232013 seconds\n",
      "\u001b[0;36m(Worker_TP0 pid=3537)\u001b[0;0m INFO 02-24 10:40:37 [gpu_worker.py:356] Available KV cache memory: 3.1 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3339)\u001b[0;0m INFO 02-24 10:40:37 [kv_cache_utils.py:1307] GPU KV cache size: 20,320 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3339)\u001b[0;0m INFO 02-24 10:40:37 [kv_cache_utils.py:1312] Maximum concurrency for 2,048 tokens per request: 9.92x\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3339)\u001b[0;0m INFO 02-24 10:40:39 [core.py:272] init engine (profile, create kv cache, warmup model) took 6.33 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3339)\u001b[0;0m INFO 02-24 10:40:41 [vllm.py:624] Asynchronous scheduling is enabled.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3339)\u001b[0;0m WARNING 02-24 10:40:41 [vllm.py:669] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3339)\u001b[0;0m INFO 02-24 10:40:41 [vllm.py:762] Cudagraph is disabled under eager mode\n",
      "INFO 02-24 10:40:41 [llm.py:343] Supported tasks: ['generate']\n",
      "\n",
      "vLLM loaded with Tensor Parallelism!\n",
      "CPU times: user 4.66 s, sys: 4.89 s, total: 9.56 s\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "print(\"Loading 70B model with vLLM Tensor Parallelism...\")\n",
    "print(\"tensor_parallel_size=2\")\n",
    "print(\"This splits MATRICES across GPUs -- both GPUs work on every layer.\")\n",
    "print()\n",
    "\n",
    "llm_tp = LLM(\n",
    "    model=model_path,\n",
    "    tensor_parallel_size=2,\n",
    "    dtype=\"float16\",\n",
    "    gpu_memory_utilization=0.92,\n",
    "    max_model_len=2048,\n",
    "    enforce_eager=True,\n",
    "    disable_custom_all_reduce=True,\n",
    ")\n",
    "\n",
    "print(\"\\nvLLM loaded with Tensor Parallelism!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with vLLM Tensor Parallelism...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687085cff7b74c3d91af7c9de8c570b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60dc7af8c7a04aa38540866b08ebc3c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A farmer has 3 fields. Field A produces twice as much wheat as Field B. Field C produces 50kg more than Field A. Together all three fields produce 550kg. How much does each field produce? Let's denote the amount of wheat produced by Field B as \\( x \\) kg.\n",
      "\n",
      "According to the problem:\n",
      "- Field A produces twice as much wheat as Field B, so Field A produces \\( 2x \\) kg.\n",
      "- Field C produces 50 kg more than Field A, so Field C produces \\( 2x + 50 \\) kg.\n",
      "- The total production from all three fields is 550 kg.\n",
      "\n",
      "We can set up the following equation to represent the total production:\n",
      "\n",
      "\\[\n",
      "x + 2x + (2x + 50) = 550\n",
      "\\]\n",
      "\n",
      "Simplify the equation:\n",
      "\n",
      "\\[\n",
      "x + 2x + 2x + 50 = 550\n",
      "\\]\n",
      "\n",
      "Combine like terms:\n",
      "\n",
      "\\[\n",
      "5x + 50 = 550\n",
      "\\]\n",
      "\n",
      "Subtract 50 from both sides:\n",
      "\n",
      "\\[\n",
      "5x = 500\n",
      "\\]\n",
      "\n",
      "Divide both sides by 5:\n",
      "\n",
      "\\[\n",
      "x = 100\n",
      "\\]\n",
      "\n",
      "Now we can find the production of each field:\n",
      "- Field B produces \\( x = 100 \\) kg.\n",
      "- Field A produces \\( 2x = 2 \\times 100 = 200 \\) kg.\n",
      "- Field C produces \\( 2x + 50 = 200 + 50 = 250 \\) kg.\n",
      "\n",
      "So, the production of each field is:\n",
      "- Field A: 200 kg\n",
      "- Field B: 100 kg\n",
      "- Field C: 250 kg\n",
      "\n",
      "To verify, we can add the amounts:\n",
      "\n",
      "\\[\n",
      "100 + 200 + 250 = 550 \\text{ kg}\n",
      "\\]\n",
      "\n",
      "The solution is correct. Each field produces:\n",
      "- Field A: 200 kg\n",
      "- Field B: 100 kg\n",
      "- Field C: 250 kg\n",
      "\n",
      "============================================================\n",
      "vLLM TENSOR PARALLEL RESULTS (2 GPUs):\n",
      "  Total time:                 22.0s\n",
      "  Tokens generated:           411\n",
      "  Speed:                      18.7 tokens/sec\n",
      "\n",
      "Correct answer: Field B = 100kg, Field A = 200kg, Field C = 250kg\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    max_tokens=2048,\n",
    ")\n",
    "\n",
    "print(f\"Generating with vLLM Tensor Parallelism...\\n\")\n",
    "\n",
    "start = time.time()\n",
    "outputs = llm_tp.generate([prompt], sampling_params)\n",
    "vllm_tp_total_time = time.time() - start\n",
    "\n",
    "generated = outputs[0].outputs[0].text\n",
    "vllm_tp_tokens = len(outputs[0].outputs[0].token_ids)\n",
    "vllm_tp_speed = vllm_tp_tokens / vllm_tp_total_time\n",
    "\n",
    "print(prompt + generated)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"vLLM TENSOR PARALLEL RESULTS (2 GPUs):\")\n",
    "print(f\"  Total time:                 {vllm_tp_total_time:.1f}s\")\n",
    "print(f\"  Tokens generated:           {vllm_tp_tokens}\")\n",
    "print(f\"  Speed:                      {vllm_tp_speed:.1f} tokens/sec\")\n",
    "print(f\"\\nCorrect answer: Field B = 100kg, Field A = 200kg, Field C = 250kg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Save TP results and restart for Pipeline Parallel\n",
    "\n",
    "We need to fully unload vLLM before loading with a different parallelism strategy.  \n",
    "**Restart the kernel**, then continue with 6.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM Tensor Parallel results saved to /workspace/vllm_tp_results.json\n",
      "  Speed: 18.7 tok/s | Total: 22.0s\n",
      "\n",
      "============================================================\n",
      "  NOW: Restart the kernel (Kernel -> Restart)\n",
      "  Then continue from 6.4 below (vLLM Pipeline Parallel)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Save vLLM TP results\n",
    "vllm_tp_results = {\n",
    "    \"vllm_tp_speed\": vllm_tp_speed,\n",
    "    \"vllm_tp_total_time\": vllm_tp_total_time,\n",
    "    \"vllm_tp_tokens\": vllm_tp_tokens,\n",
    "}\n",
    "with open(\"/workspace/vllm_tp_results.json\", \"w\") as f:\n",
    "    json.dump(vllm_tp_results, f)\n",
    "\n",
    "print(\"vLLM Tensor Parallel results saved to /workspace/vllm_tp_results.json\")\n",
    "print(f\"  Speed: {vllm_tp_speed:.1f} tok/s | Total: {vllm_tp_total_time:.1f}s\")\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"  NOW: Restart the kernel (Kernel -> Restart)\")\n",
    "print(\"  Then continue from 6.4 below (vLLM Pipeline Parallel)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 vLLM Pipeline Parallel (2 GPUs)\n",
    "\n",
    "Now let's run vLLM with **Pipeline Parallelism** -- layers split across GPUs.  \n",
    "**Same engine, same model, different parallelism strategy.** Fair comparison.\n",
    "\n",
    "> **If you restarted the kernel**, start running from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded previous results:\n",
      "  HF Pipeline:   8.7 tok/s | 409 tokens\n",
      "  HF Quantized:  12.3 tok/s | 408 tokens\n",
      "  vLLM Tensor:   18.7 tok/s | 411 tokens\n",
      "\n",
      "Loading 70B model with vLLM Pipeline Parallelism...\n",
      "pipeline_parallel_size=2, tensor_parallel_size=1\n",
      "This splits LAYERS across GPUs (like HuggingFace, but with vLLM's engine).\n",
      "\n",
      "INFO 02-24 10:44:32 [utils.py:261] non-default args: {'dtype': 'float16', 'max_model_len': 2048, 'pipeline_parallel_size': 2, 'disable_log_stats': True, 'enforce_eager': True, 'model': '/workspace/models/models--Qwen--Qwen2.5-72B-Instruct/snapshots/495f39366efef23836d0cfae4fbe635880d2be31'}\n",
      "INFO 02-24 10:44:32 [model.py:541] Resolved architecture: Qwen2ForCausalLM\n",
      "WARNING 02-24 10:44:32 [model.py:1885] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 02-24 10:44:32 [model.py:1561] Using max model len 2048\n",
      "INFO 02-24 10:44:33 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 02-24 10:44:33 [vllm.py:624] Asynchronous scheduling is enabled.\n",
      "WARNING 02-24 10:44:33 [vllm.py:662] Enforce eager set, overriding optimization level to -O0\n",
      "INFO 02-24 10:44:33 [vllm.py:762] Cudagraph is disabled under eager mode\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4110)\u001b[0;0m INFO 02-24 10:44:33 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/workspace/models/models--Qwen--Qwen2.5-72B-Instruct/snapshots/495f39366efef23836d0cfae4fbe635880d2be31', speculative_config=None, tokenizer='/workspace/models/models--Qwen--Qwen2.5-72B-Instruct/snapshots/495f39366efef23836d0cfae4fbe635880d2be31', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=2, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/workspace/models/models--Qwen--Qwen2.5-72B-Instruct/snapshots/495f39366efef23836d0cfae4fbe635880d2be31, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4110)\u001b[0;0m WARNING 02-24 10:44:33 [multiproc_executor.py:910] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4110)\u001b[0;0m INFO 02-24 10:44:37 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:37995 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4110)\u001b[0;0m INFO 02-24 10:44:37 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:37995 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4110)\u001b[0;0m INFO 02-24 10:44:37 [pynccl.py:111] vLLM is using nccl==2.27.5\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4110)\u001b[0;0m INFO 02-24 10:44:38 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4110)\u001b[0;0m INFO 02-24 10:44:38 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 1, PCP rank 0, TP rank 0, EP rank N/A\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4110)\u001b[0;0m \u001b[0;36m(Worker_PP0 pid=4124)\u001b[0;0m INFO 02-24 10:44:38 [gpu_model_runner.py:4033] Starting to load model /workspace/models/models--Qwen--Qwen2.5-72B-Instruct/snapshots/495f39366efef23836d0cfae4fbe635880d2be31...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4110)\u001b[0;0m \u001b[0;36m(Worker_PP0 pid=4124)\u001b[0;0m INFO 02-24 10:44:40 [cuda.py:364] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0f32e6f4a4428ab33575527d0d0359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/37 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=4110)\u001b[0;0m \u001b[0;36m(Worker_PP0 pid=4124)\u001b[0;0m INFO 02-24 10:45:45 [default_loader.py:291] Loading weights took 65.31 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4110)\u001b[0;0m \u001b[0;36m(Worker_PP0 pid=4124)\u001b[0;0m INFO 02-24 10:45:46 [gpu_model_runner.py:4130] Model loading took 67.72 GiB memory and 66.732011 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4110)\u001b[0;0m \u001b[0;36m(Worker_PP0 pid=4124)\u001b[0;0m INFO 02-24 10:45:53 [gpu_worker.py:356] Available KV cache memory: 1.73 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4110)\u001b[0;0m INFO 02-24 10:45:53 [kv_cache_utils.py:1307] GPU KV cache size: 10,496 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4110)\u001b[0;0m INFO 02-24 10:45:53 [kv_cache_utils.py:1312] Maximum concurrency for 2,048 tokens per request: 5.12x\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4110)\u001b[0;0m INFO 02-24 10:45:53 [core.py:272] init engine (profile, create kv cache, warmup model) took 3.30 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4110)\u001b[0;0m WARNING 02-24 10:45:55 [vllm.py:669] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4110)\u001b[0;0m INFO 02-24 10:45:55 [vllm.py:762] Cudagraph is disabled under eager mode\n",
      "INFO 02-24 10:45:55 [llm.py:343] Supported tasks: ['generate']\n",
      "\n",
      "vLLM loaded with Pipeline Parallelism!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=4110)\u001b[0;0m \u001b[0;36m(Worker_PP0 pid=4124)\u001b[0;0m /usr/local/lib/python3.11/dist-packages/vllm/distributed/parallel_state.py:650: UserWarning: The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1581.)\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4110)\u001b[0;0m \u001b[0;36m(Worker_PP0 pid=4124)\u001b[0;0m   object_tensor = torch.frombuffer(pickle.dumps(obj), dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "import torch, time, json\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Reload all previous results\n",
    "with open(\"/workspace/hf_results.json\", \"r\") as f:\n",
    "    hf = json.load(f)\n",
    "with open(\"/workspace/vllm_tp_results.json\", \"r\") as f:\n",
    "    vtp = json.load(f)\n",
    "\n",
    "pp_speed = hf[\"pp_speed\"]\n",
    "pp_total_time = hf[\"pp_total_time\"]\n",
    "pp_tokens = hf[\"pp_tokens\"]\n",
    "q_speed = hf[\"q_speed\"]\n",
    "q_total_time = hf[\"q_total_time\"]\n",
    "q_tokens = hf[\"q_tokens\"]\n",
    "model_path = hf[\"model_path\"]\n",
    "vllm_tp_speed = vtp[\"vllm_tp_speed\"]\n",
    "vllm_tp_total_time = vtp[\"vllm_tp_total_time\"]\n",
    "vllm_tp_tokens = vtp[\"vllm_tp_tokens\"]\n",
    "\n",
    "print(f\"Loaded previous results:\")\n",
    "print(f\"  HF Pipeline:   {pp_speed:.1f} tok/s | {pp_tokens} tokens\")\n",
    "print(f\"  HF Quantized:  {q_speed:.1f} tok/s | {q_tokens} tokens\")\n",
    "print(f\"  vLLM Tensor:   {vllm_tp_speed:.1f} tok/s | {vllm_tp_tokens} tokens\")\n",
    "print()\n",
    "\n",
    "prompt = \"A farmer has 3 fields. Field A produces twice as much wheat as Field B. Field C produces 50kg more than Field A. Together all three fields produce 550kg. How much does each field produce?\"\n",
    "\n",
    "print(\"Loading 70B model with vLLM Pipeline Parallelism...\")\n",
    "print(\"pipeline_parallel_size=2, tensor_parallel_size=1\")\n",
    "print(\"This splits LAYERS across GPUs (like HuggingFace, but with vLLM's engine).\")\n",
    "print()\n",
    "\n",
    "llm_pp = LLM(\n",
    "    model=model_path,\n",
    "    pipeline_parallel_size=2,\n",
    "    tensor_parallel_size=1,\n",
    "    dtype=\"float16\",\n",
    "    max_model_len=2048,\n",
    "    enforce_eager=True,\n",
    ")\n",
    "\n",
    "print(\"\\nvLLM loaded with Pipeline Parallelism!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with vLLM Pipeline Parallelism...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9a69c849494a6d9b323ebd4afcddb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e610c33c9b0b40b598290b295801f64a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W224 10:45:55.605651321 ProcessGroupNCCL.cpp:4004] Warning: An unbatched P2P op (send/recv) was called on this ProcessGroup with size 2.  In lazy initialization mode, this will result in a new 2-rank NCCL communicator to be created. (function operator())\n",
      "[rank1]:[W224 10:45:55.606222328 ProcessGroupNCCL.cpp:4004] Warning: An unbatched P2P op (send/recv) was called on this ProcessGroup with size 2.  In lazy initialization mode, this will result in a new 2-rank NCCL communicator to be created. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A farmer has 3 fields. Field A produces twice as much wheat as Field B. Field C produces 50kg more than Field A. Together all three fields produce 550kg. How much does each field produce? Let's denote the amount of wheat produced by Field B as \\( x \\) kg.\n",
      "\n",
      "According to the problem:\n",
      "- Field A produces twice as much wheat as Field B, so Field A produces \\( 2x \\) kg.\n",
      "- Field C produces 50 kg more than Field A, so Field C produces \\( 2x + 50 \\) kg.\n",
      "- The total production from all three fields is 550 kg.\n",
      "\n",
      "We can set up the following equation to represent the total production:\n",
      "\n",
      "\\[\n",
      "x + 2x + (2x + 50) = 550\n",
      "\\]\n",
      "\n",
      "Simplify the equation:\n",
      "\n",
      "\\[\n",
      "x + 2x + 2x + 50 = 550\n",
      "\\]\n",
      "\n",
      "Combine like terms:\n",
      "\n",
      "\\[\n",
      "5x + 50 = 550\n",
      "\\]\n",
      "\n",
      "Subtract 50 from both sides:\n",
      "\n",
      "\\[\n",
      "5x = 500\n",
      "\\]\n",
      "\n",
      "Divide both sides by 5:\n",
      "\n",
      "\\[\n",
      "x = 100\n",
      "\\]\n",
      "\n",
      "Now we can find the production of each field:\n",
      "- Field B produces \\( x = 100 \\) kg.\n",
      "- Field A produces \\( 2x = 2 \\times 100 = 200 \\) kg.\n",
      "- Field C produces \\( 2x + 50 = 200 + 50 = 250 \\) kg.\n",
      "\n",
      "So, the production of each field is:\n",
      "- Field A: 200 kg\n",
      "- Field B: 100 kg\n",
      "- Field C: 250 kg\n",
      "\n",
      "To verify, we can add the amounts:\n",
      "\n",
      "\\[\n",
      "100 + 200 + 250 = 550 \\text{ kg}\n",
      "\\]\n",
      "\n",
      "The solution is correct. Each field produces:\n",
      "- Field A: 200 kg\n",
      "- Field B: 100 kg\n",
      "- Field C: 250 kg\n",
      "\n",
      "============================================================\n",
      "vLLM PIPELINE PARALLEL RESULTS (2 GPUs):\n",
      "  Total time:                 38.5s\n",
      "  Tokens generated:           411\n",
      "  Speed:                      10.7 tokens/sec\n",
      "\n",
      "Correct answer: Field B = 100kg, Field A = 200kg, Field C = 250kg\n",
      "\n",
      "FAIR COMPARISON (same engine -- vLLM):\n",
      "  vLLM Tensor Parallel:     18.7 tok/s  (411 tokens in 22.0s)\n",
      "  vLLM Pipeline Parallel:   10.7 tok/s  (411 tokens in 38.5s)\n",
      "  TP speedup over PP:     1.75x\n",
      "\n",
      "Same model, same engine, same prompt -- the ONLY difference is the parallelism strategy.\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    max_tokens=2048,\n",
    ")\n",
    "\n",
    "print(f\"Generating with vLLM Pipeline Parallelism...\\n\")\n",
    "\n",
    "start = time.time()\n",
    "outputs = llm_pp.generate([prompt], sampling_params)\n",
    "vllm_pp_total_time = time.time() - start\n",
    "\n",
    "generated = outputs[0].outputs[0].text\n",
    "vllm_pp_tokens = len(outputs[0].outputs[0].token_ids)\n",
    "vllm_pp_speed = vllm_pp_tokens / vllm_pp_total_time\n",
    "\n",
    "print(prompt + generated)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"vLLM PIPELINE PARALLEL RESULTS (2 GPUs):\")\n",
    "print(f\"  Total time:                 {vllm_pp_total_time:.1f}s\")\n",
    "print(f\"  Tokens generated:           {vllm_pp_tokens}\")\n",
    "print(f\"  Speed:                      {vllm_pp_speed:.1f} tokens/sec\")\n",
    "print(f\"\\nCorrect answer: Field B = 100kg, Field A = 200kg, Field C = 250kg\")\n",
    "print()\n",
    "print(f\"FAIR COMPARISON (same engine -- vLLM):\")\n",
    "print(f\"  vLLM Tensor Parallel:   {vllm_tp_speed:>6.1f} tok/s  ({vllm_tp_tokens} tokens in {vllm_tp_total_time:.1f}s)\")\n",
    "print(f\"  vLLM Pipeline Parallel: {vllm_pp_speed:>6.1f} tok/s  ({vllm_pp_tokens} tokens in {vllm_pp_total_time:.1f}s)\")\n",
    "print(f\"  TP speedup over PP:     {vllm_tp_speed/vllm_pp_speed:.2f}x\")\n",
    "print()\n",
    "print(\"Same model, same engine, same prompt -- the ONLY difference is the parallelism strategy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: The Complete Picture\n",
    "\n",
    "Let's see all our results side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE COMPARISON: Four ways to run a 70B model\n",
      "================================================================================\n",
      "                                HF Pipeline      HF INT4      vLLM TP      vLLM PP\n",
      "--------------------------------------------------------------------------------\n",
      "GPUs used                                 2            1            2            2\n",
      "Parallelism                        Pipeline         None       Tensor     Pipeline\n",
      "Framework                       HuggingFace  HuggingFace         vLLM         vLLM\n",
      "Precision                              FP16         INT4         FP16         FP16\n",
      "Speed (tok/s)                           8.7         12.3         18.7         10.7\n",
      "Tokens generated                        409          408          411          411\n",
      "Total time                            47.0s        33.2s        22.0s        38.5s\n",
      "Quality                                Full  Slight loss         Full         Full\n",
      "Memory per GPU                       ~70 GB       ~35 GB       ~66 GB       ~70 GB\n",
      "================================================================================\n",
      "\n",
      "FAIR COMPARISON (same engine, same model, same prompt):\n",
      "  vLLM Tensor Parallel vs vLLM Pipeline Parallel: 1.75x speedup\n",
      "\n",
      "KEY TAKEAWAYS:\n",
      "  1. Tensor Parallelism is faster than Pipeline Parallelism (both GPUs work on every layer)\n",
      "  2. The vLLM TP vs PP comparison is FAIR (same engine) -- proves TP's advantage\n",
      "  3. HF vs vLLM comparison shows engine optimization matters too (PagedAttention, fused kernels)\n",
      "  4. Quantization trades quality for efficiency -- fits 70B on 1 GPU\n",
      "  5. Real systems combine all 3: TP within server, PP across servers, DP for throughput\n",
      "\n",
      "3D PARALLELISM in production:\n",
      "  TP(8 GPUs/server) x PP(4 servers) x DP(33 copies) = 1,056 GPUs\n",
      "  This is how models like Claude and GPT are trained and served.\n"
     ]
    }
   ],
   "source": [
    "print(\"COMPLETE COMPARISON: Four ways to run a 70B model\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'':30s} {'HF Pipeline':>12s} {'HF INT4':>12s} {'vLLM TP':>12s} {'vLLM PP':>12s}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'GPUs used':30s} {'2':>12s} {'1':>12s} {'2':>12s} {'2':>12s}\")\n",
    "print(f\"{'Parallelism':30s} {'Pipeline':>12s} {'None':>12s} {'Tensor':>12s} {'Pipeline':>12s}\")\n",
    "print(f\"{'Framework':30s} {'HuggingFace':>12s} {'HuggingFace':>12s} {'vLLM':>12s} {'vLLM':>12s}\")\n",
    "print(f\"{'Precision':30s} {'FP16':>12s} {'INT4':>12s} {'FP16':>12s} {'FP16':>12s}\")\n",
    "print(f\"{'Speed (tok/s)':30s} {pp_speed:>12.1f} {q_speed:>12.1f} {vllm_tp_speed:>12.1f} {vllm_pp_speed:>12.1f}\")\n",
    "print(f\"{'Tokens generated':30s} {pp_tokens:>12d} {q_tokens:>12d} {vllm_tp_tokens:>12d} {vllm_pp_tokens:>12d}\")\n",
    "print(f\"{'Total time':30s} {pp_total_time:>11.1f}s {q_total_time:>11.1f}s {vllm_tp_total_time:>11.1f}s {vllm_pp_total_time:>11.1f}s\")\n",
    "print(f\"{'Quality':30s} {'Full':>12s} {'Slight loss':>12s} {'Full':>12s} {'Full':>12s}\")\n",
    "print(f\"{'Memory per GPU':30s} {'~70 GB':>12s} {'~35 GB':>12s} {'~66 GB':>12s} {'~70 GB':>12s}\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"FAIR COMPARISON (same engine, same model, same prompt):\")\n",
    "print(f\"  vLLM Tensor Parallel vs vLLM Pipeline Parallel: {vllm_tp_speed/vllm_pp_speed:.2f}x speedup\")\n",
    "print()\n",
    "print(\"KEY TAKEAWAYS:\")\n",
    "print(\"  1. Tensor Parallelism is faster than Pipeline Parallelism (both GPUs work on every layer)\")\n",
    "print(\"  2. The vLLM TP vs PP comparison is FAIR (same engine) -- proves TP's advantage\")\n",
    "print(\"  3. HF vs vLLM comparison shows engine optimization matters too (PagedAttention, fused kernels)\")\n",
    "print(\"  4. Quantization trades quality for efficiency -- fits 70B on 1 GPU\")\n",
    "print(\"  5. Real systems combine all 3: TP within server, PP across servers, DP for throughput\")\n",
    "print()\n",
    "print(\"3D PARALLELISM in production:\")\n",
    "print(\"  TP(8 GPUs/server) x PP(4 servers) x DP(33 copies) = 1,056 GPUs\")\n",
    "print(\"  This is how models like Claude and GPT are trained and served.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "We started with a problem: **a 144GB model that doesn't fit on an 80GB GPU.**\n",
    "\n",
    "We solved it three ways:\n",
    "\n",
    "1. **Pipeline Parallelism** -- Split layers across GPUs. Simple but has idle time (pipeline bubbles).\n",
    "2. **Quantization** -- Shrink the numbers from 16-bit to 4-bit. Fits on 1 GPU, slight quality loss.\n",
    "3. **Tensor Parallelism** -- Split matrices across GPUs. Faster, but needs fast interconnect (NVLink).\n",
    "\n",
    "Then we did a **fair comparison** using vLLM (same engine, different strategies) and confirmed that **Tensor Parallelism is faster than Pipeline Parallelism**.\n",
    "\n",
    "And we learned about **Data Parallelism** -- replicating the model to train on more data in parallel, with gradient synchronization (AllReduce).\n",
    "\n",
    "In real production (Claude, ChatGPT, etc.), all techniques are combined as **3D Parallelism**:\n",
    "- **Tensor Parallel** within a server (NVLink)\n",
    "- **Pipeline Parallel** across servers (InfiniBand)\n",
    "- **Data Parallel** for throughput (gradient sync)\n",
    "- **Quantization** to reduce cost\n",
    "\n",
    "---\n",
    "\n",
    "*Hardware: 2x NVIDIA A100 SXM4 80GB on RunPod*  \n",
    "*Model: Qwen2.5 72B Instruct (72B parameters)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
